import os
import cv2
import numpy as np
import insightface
from tensorflow.keras.models import load_model
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense
import tensorflow as tf
import time
import datetime
import json
import shutil
import gc  # ガベージコレクション用

# ---------------------------------------------
# パス設定と初期化
# ---------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TEST_VIDEOS_DIR = os.path.join(BASE_DIR, "test_videos")
RESULT_VIDEOS_DIR = os.path.join(BASE_DIR, "results_videos")
NO_FACE_VIDEOS_DIR = os.path.join(BASE_DIR, "unnecessary_file/test_no_face_detected")
DATASET_DIR = os.path.join(BASE_DIR, "processed_data")
MODEL_DIR = os.path.join(BASE_DIR, "main_model")
MODEL_PATH = os.path.join(MODEL_DIR, "combined_model_saved")  # TensorFlow SavedModel形式のパス

if not os.path.exists(RESULT_VIDEOS_DIR):
    os.makedirs(RESULT_VIDEOS_DIR)
if not os.path.exists(NO_FACE_VIDEOS_DIR):
    os.makedirs(NO_FACE_VIDEOS_DIR)

# ArcFaceとEfficientNetV2の読み込み
print("Loading ArcFace model...")
app = insightface.app.FaceAnalysis(name='buffalo_l')
app.prepare(ctx_id=-1, det_size=(224, 224))

print("Loading EfficientNetV2 model for feature extraction...")
base_efficientnet = EfficientNetV2S(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
x = base_efficientnet.output
efficientnet_output = tf.keras.layers.GlobalAveragePooling2D()(x)
efficientnet_output = Dense(512, activation='relu')(efficientnet_output)  # 次元を512に統一
efficientnet_model = Model(inputs=base_efficientnet.input, outputs=efficientnet_output)

print("Loading trained model...")
model = tf.keras.models.load_model(MODEL_PATH)  # TensorFlow SavedModel形式でロード

class_indices_path = os.path.join(MODEL_DIR, "class_indices.json")
with open(class_indices_path, "r") as f:
    class_indices = json.load(f)
class_names = [class_indices[str(i)] for i in range(len(class_indices))]
print(f"Loaded class names: {class_names}")

# ---------------------------------------------
# フレーム処理設定
# ---------------------------------------------
# 処理するフレーム数とスキップするフレーム数を指定
PROCESS_FRAMES = 6
SKIP_FRAMES = 6

# ---------------------------------------------
# 画像の前処理関数
# ---------------------------------------------
def extract_embeddings(image, face):
    """
    顔領域からArcFaceとEfficientNetV2の埋め込みベクトルを取得
    """
    try:
        arcface_embedding = face.embedding
        x1, y1, x2, y2 = face.bbox.astype(int)
        h, w, _ = image.shape
        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w, x2), min(h, y2)
        face_img = image[y1:y2, x1:x2]
        if face_img.size == 0:
            return None, None
        face_resized = cv2.resize(face_img, (224, 224))
        face_array = np.expand_dims(face_resized / 255.0, axis=0)
        efficientnet_embedding = efficientnet_model.predict(face_array, verbose=0).flatten()
        return arcface_embedding, efficientnet_embedding
    except Exception as e:
        print(f"Error during embedding extraction: {e}")
        return None, None

# ---------------------------------------------
# 動画の推論処理
# ---------------------------------------------
print("Starting inference on test videos...")
UNKNOWN_THRESHOLD = 80.0
MIN_FONT_SCALE = 0.8
MAX_FONT_SCALE = 3.5

for video_name in os.listdir(TEST_VIDEOS_DIR):
    test_video_path = os.path.join(TEST_VIDEOS_DIR, video_name)
    cap = cv2.VideoCapture(test_video_path)
    if not cap.isOpened():
        print(f"Unable to open video: {test_video_path}")
        continue

    # 結果動画の保存先
    result_video_path = os.path.join(RESULT_VIDEOS_DIR, f"result_{video_name}")
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # 総フレーム数
    out = cv2.VideoWriter(result_video_path, fourcc, fps, (frame_width, frame_height))

    no_face_detected = True
    frame_idx = 0
    start_time = time.time()  # 処理開始時間を記録

    # 処理状態を切り替えるためのフラグ
    process_mode = True  # True: 処理モード, False: スキップモード
    process_counter = 0  # 現在の処理フレーム数またはスキップフレーム数

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # 処理モードで顔検出・認識を実行
        if process_mode:
            faces = app.get(frame)  # 顔検出
            if len(faces) > 0:
                no_face_detected = False

            for face_idx, face in enumerate(faces, start=1):
                arcface_embedding, efficientnet_embedding = extract_embeddings(frame, face)
                if arcface_embedding is None or efficientnet_embedding is None:
                    continue

                embeddings_input = [np.expand_dims(arcface_embedding, axis=0), np.expand_dims(efficientnet_embedding, axis=0)]
                predictions = model.predict(embeddings_input, verbose=0)

                predicted_class_index = np.argmax(predictions)
                confidence = predictions[0][predicted_class_index] * 100

                if confidence < UNKNOWN_THRESHOLD:
                    predicted_class = "unknown"
                else:
                    predicted_class = class_names[predicted_class_index]

                x1, y1, x2, y2 = face.bbox.astype(int)
                label = f"{predicted_class} ({confidence:.2f}%)"
                color = (0, 255, 0) if predicted_class != "unknown" else (0, 0, 255)
                font_scale = max(frame_width, frame_height) / 1200
                font_scale = max(MIN_FONT_SCALE, min(MAX_FONT_SCALE, font_scale))
                thickness = max(1, int(font_scale * 2))

                cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)
                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)

        # 処理状態を管理
        process_counter += 1
        if process_mode and process_counter >= PROCESS_FRAMES:
            process_mode = False
            process_counter = 0
        elif not process_mode and process_counter >= SKIP_FRAMES:
            process_mode = True
            process_counter = 0

        out.write(frame)  # 全フレームを結果動画に保存

        # フレームの削除
        del frame  # メモリ解放
        gc.collect()  # ガベージコレクションを実行

        frame_idx += 1

        # 進捗と残り時間の計算
        elapsed_time = time.time() - start_time
        progress = frame_idx / total_frames  # 進捗率
        if progress > 0:
            total_estimated_time = elapsed_time / progress
            remaining_time = total_estimated_time - elapsed_time
        else:
            remaining_time = 0  # 初期状態で進捗がない場合

        # 残り時間を hh:mm:ss フォーマットに変換
        remaining_time_str = str(datetime.timedelta(seconds=int(remaining_time)))

        # コンソールに進捗と残り時間を表示
        print(f"Processing {video_name}: {frame_idx}/{total_frames} frames "
              f"({progress * 100:.2f}%) - "
              f"Elapsed Time: {elapsed_time:.2f}s - "
              f"Remaining Time: {remaining_time_str}", end="\r")

    cap.release()
    out.release()

    if no_face_detected:
        no_face_video_path = os.path.join(NO_FACE_VIDEOS_DIR, video_name)
        shutil.move(test_video_path, no_face_video_path)
        print(f"\nNo faces detected. Moved video to: {no_face_video_path}")
    else:
        print(f"\nResults saved to: {result_video_path}")

    # ガベージコレクションを実行
    gc.collect()
