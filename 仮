import os
import cv2
import numpy as np
import sys
import shutil
import random
from tensorflow.keras.preprocessing.image import ImageDataGenerator

def confirm_and_reset_output_dir(output_dir):
    """
    processed_dataフォルダが存在する場合、ユーザーに確認して内容を削除する。
    """
    if os.path.exists(output_dir) and os.listdir(output_dir):
        response = input(f"Warning: '{output_dir}' already exists and contains data. Do you want to reset it? (yes/no): ").strip().lower()
        if response == 'yes':
            print("Resetting processed_data folder...")
            shutil.rmtree(output_dir)  # フォルダ削除
            os.makedirs(output_dir, exist_ok=True)  # 新たに作成
        else:
            print("Aborting process.")
            sys.exit(0)
    else:
        os.makedirs(output_dir, exist_ok=True)

def organize_and_resize_with_blurred_background(input_dir, output_dir, no_face_dir, target_size=(112, 112)):
    """
    顔検出を行い、顔が検出できた画像をリサイズして保存。
    顔が検出できなかった画像はno_face_dataフォルダに保存。
    """
    cascades_dir = os.path.join(os.path.dirname(__file__), "cascades")
    model_file = os.path.join(cascades_dir, "deploy.prototxt")
    weights_file = os.path.join(cascades_dir, "res10_300x300_ssd_iter_140000.caffemodel")
    net = cv2.dnn.readNetFromCaffe(model_file, weights_file)

    os.makedirs(no_face_dir, exist_ok=True)

    total_images = sum(len(files) for _, _, files in os.walk(input_dir))
    processed_images = 0
    no_face_count = 0

    for class_name in os.listdir(input_dir):
        class_path = os.path.join(input_dir, class_name)
        if not os.path.isdir(class_path):
            continue

        output_class_path = os.path.join(output_dir, class_name)
        os.makedirs(output_class_path, exist_ok=True)

        for img_name in os.listdir(class_path):
            img_path = os.path.join(class_path, img_name)
            img = cv2.imread(img_path)

            if img is None:
                print(f"Warning: Unable to read image {img_path}")
                continue

            (h, w) = img.shape[:2]
            blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
            net.setInput(blob)
            detections = net.forward()

            max_area = 0
            face_box = None
            for i in range(detections.shape[2]):
                confidence = detections[0, 0, i, 2]
                if confidence > 0.5:
                    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                    (x1, y1, x2, y2) = box.astype("int")
                    area = (x2 - x1) * (y2 - y1)
                    if area > max_area:
                        max_area = area
                        face_box = (x1, y1, x2, y2)

            if face_box is not None:
                processed_img = blur_background(img, face_box, target_size)
                output_path = os.path.join(output_class_path, img_name)
                cv2.imwrite(output_path, processed_img)
            else:
                no_face_count += 1
                no_face_path = os.path.join(no_face_dir, img_name)
                cv2.imwrite(no_face_path, img)

            processed_images += 1
            progress = f"Processing: {processed_images}/{total_images} ({(processed_images / total_images) * 100:.2f}%) - No Face: {no_face_count}"
            sys.stdout.write(f"\r{progress}")
            sys.stdout.flush()

    print(f"\nProcessed data saved to: {output_dir}")
    print(f"No face detected images saved to: {no_face_dir}")
    print(f"Total images: {total_images}, No face detected: {no_face_count}")

def blur_background(image, face_box, target_size):
    x1, y1, x2, y2 = face_box
    face = image[max(0, y1):min(y2, image.shape[0]), max(0, x1):min(x2, image.shape[1])]
    blurred_image = cv2.GaussianBlur(image, (51, 51), 0)
    blurred_image[max(0, y1):min(y2, image.shape[0]), max(0, x1):min(x2, image.shape[1])] = face
    return cv2.resize(blurred_image, target_size)

def augment_data(input_dir, target_count=200):
    """
    各クラスのフォルダに対してランダムな画像とランダムな方法でデータ拡張を行い、合計200枚にする。
    """
    datagen = ImageDataGenerator(
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    class_dirs = [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]
    total_classes = len(class_dirs)

    for class_idx, class_name in enumerate(class_dirs, start=1):
        class_path = os.path.join(input_dir, class_name)
        images = [os.path.join(class_path, img) for img in os.listdir(class_path)]
        num_existing = len(images)
        num_to_generate = target_count - num_existing

        generated = 0
        while generated < num_to_generate:
            img_path = random.choice(images)  # 使用する画像をランダムに選択
            img = cv2.imread(img_path)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # RGBに変換
            img = img.astype('float32') / 255.0
            img = np.expand_dims(img, axis=0)

            for batch in datagen.flow(img, batch_size=1, save_to_dir=class_path,
                                      save_prefix=f"aug_{os.path.basename(img_path).split('.')[0]}",
                                      save_format='png'):
                generated += 1
                progress = f"Class {class_idx}/{total_classes} - Augmenting '{class_name}': {generated}/{num_to_generate}"
                sys.stdout.write(f"\r{progress}")
                sys.stdout.flush()
                if generated >= num_to_generate:
                    break

        print(f"\nCompleted augmentation for class '{class_name}'. Total images: {len(os.listdir(class_path))}")

if __name__ == "__main__":
    base_dir = os.path.dirname(os.path.abspath(__file__))

    input_dir = os.path.join(base_dir, "dataset")
    processed_dir = os.path.join(base_dir, "processed_data")
    no_face_dir = os.path.join(base_dir, "no_face_data")

    # 確認とリセット
    confirm_and_reset_output_dir(processed_dir)

    # 画像のリサイズと処理
    organize_and_resize_with_blurred_background(input_dir, processed_dir, no_face_dir, target_size=(112, 112))

    # データ拡張
    augment_data(processed_dir, target_count=200)
